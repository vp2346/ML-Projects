{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting model with accuracy of 97.8%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 4s - loss: 0.4613 - acc: 0.8765 - val_loss: 0.2335 - val_acc: 0.9349\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 3s - loss: 0.2281 - acc: 0.9348 - val_loss: 0.1788 - val_acc: 0.9510\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 3s - loss: 0.1813 - acc: 0.9489 - val_loss: 0.1465 - val_acc: 0.9598\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 3s - loss: 0.1543 - acc: 0.9567 - val_loss: 0.1274 - val_acc: 0.9646\n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 3s - loss: 0.1358 - acc: 0.9617 - val_loss: 0.1121 - val_acc: 0.9684\n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 3s - loss: 0.1220 - acc: 0.9660 - val_loss: 0.0990 - val_acc: 0.9718\n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 3s - loss: 0.1119 - acc: 0.9685 - val_loss: 0.0949 - val_acc: 0.9723\n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 3s - loss: 0.1039 - acc: 0.9707 - val_loss: 0.0859 - val_acc: 0.9740\n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 3s - loss: 0.0979 - acc: 0.9726 - val_loss: 0.0807 - val_acc: 0.9752\n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 3s - loss: 0.0921 - acc: 0.9736 - val_loss: 0.0781 - val_acc: 0.9756\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 3s - loss: 0.0883 - acc: 0.9753 - val_loss: 0.0741 - val_acc: 0.9776\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 3s - loss: 0.0844 - acc: 0.9761 - val_loss: 0.0703 - val_acc: 0.9780\n",
      "Test loss: 0.070269546042\n",
      "Test accuracy: 0.978\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 12\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "    \n",
    "#Changes the values of x_train to float numbers .\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "#normalize train and test data\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Model with accuracy of 99.63% by applying image augmentation\n",
    "\n",
    "## Model contains 3 convolutional layers with 3x3 filters, 2 pooling layers with 2x2 stride, and 2 dense layers with 25% and 30% dropout rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "Epoch 1/25\n",
      "937/937 [==============================] - 22s - loss: 0.4361 - acc: 0.8590 - val_loss: 0.0644 - val_acc: 0.9794\n",
      "Epoch 2/25\n",
      "937/937 [==============================] - 20s - loss: 0.1512 - acc: 0.9526 - val_loss: 0.0420 - val_acc: 0.9880\n",
      "Epoch 3/25\n",
      "937/937 [==============================] - 20s - loss: 0.1128 - acc: 0.9642 - val_loss: 0.0312 - val_acc: 0.9900\n",
      "Epoch 4/25\n",
      "937/937 [==============================] - 20s - loss: 0.0957 - acc: 0.9707 - val_loss: 0.0323 - val_acc: 0.9896\n",
      "Epoch 5/25\n",
      "937/937 [==============================] - 20s - loss: 0.0852 - acc: 0.9737 - val_loss: 0.0240 - val_acc: 0.9913\n",
      "Epoch 6/25\n",
      "937/937 [==============================] - 20s - loss: 0.0750 - acc: 0.9771 - val_loss: 0.0340 - val_acc: 0.9904\n",
      "Epoch 7/25\n",
      "937/937 [==============================] - 20s - loss: 0.0710 - acc: 0.9787 - val_loss: 0.0257 - val_acc: 0.9921\n",
      "Epoch 8/25\n",
      "937/937 [==============================] - 20s - loss: 0.0676 - acc: 0.9798 - val_loss: 0.0271 - val_acc: 0.9929\n",
      "Epoch 9/25\n",
      "937/937 [==============================] - 20s - loss: 0.0647 - acc: 0.9810 - val_loss: 0.0263 - val_acc: 0.9924\n",
      "Epoch 10/25\n",
      "937/937 [==============================] - 20s - loss: 0.0599 - acc: 0.9823 - val_loss: 0.0205 - val_acc: 0.9935\n",
      "Epoch 11/25\n",
      "937/937 [==============================] - 20s - loss: 0.0577 - acc: 0.9824 - val_loss: 0.0273 - val_acc: 0.9923\n",
      "Epoch 12/25\n",
      "937/937 [==============================] - 20s - loss: 0.0578 - acc: 0.9826 - val_loss: 0.0286 - val_acc: 0.9913\n",
      "Epoch 13/25\n",
      "937/937 [==============================] - 20s - loss: 0.0554 - acc: 0.9836 - val_loss: 0.0242 - val_acc: 0.9929\n",
      "Epoch 14/25\n",
      "937/937 [==============================] - 20s - loss: 0.0567 - acc: 0.9836 - val_loss: 0.0206 - val_acc: 0.9932\n",
      "Epoch 15/25\n",
      "937/937 [==============================] - 20s - loss: 0.0499 - acc: 0.9852 - val_loss: 0.0206 - val_acc: 0.9939\n",
      "Epoch 16/25\n",
      "937/937 [==============================] - 20s - loss: 0.0511 - acc: 0.9846 - val_loss: 0.0236 - val_acc: 0.9932\n",
      "Epoch 17/25\n",
      "937/937 [==============================] - 20s - loss: 0.0495 - acc: 0.9857 - val_loss: 0.0171 - val_acc: 0.9949\n",
      "Epoch 18/25\n",
      "937/937 [==============================] - 20s - loss: 0.0524 - acc: 0.9847 - val_loss: 0.0201 - val_acc: 0.9935\n",
      "Epoch 19/25\n",
      "937/937 [==============================] - 20s - loss: 0.0493 - acc: 0.9855 - val_loss: 0.0212 - val_acc: 0.9948\n",
      "Epoch 20/25\n",
      "937/937 [==============================] - 20s - loss: 0.0474 - acc: 0.9862 - val_loss: 0.0232 - val_acc: 0.9935\n",
      "Epoch 21/25\n",
      "937/937 [==============================] - 20s - loss: 0.0462 - acc: 0.9865 - val_loss: 0.0196 - val_acc: 0.9941\n",
      "Epoch 22/25\n",
      "937/937 [==============================] - 20s - loss: 0.0454 - acc: 0.9868 - val_loss: 0.0212 - val_acc: 0.9940\n",
      "Epoch 23/25\n",
      "937/937 [==============================] - 20s - loss: 0.0461 - acc: 0.9871 - val_loss: 0.0179 - val_acc: 0.9953\n",
      "Epoch 24/25\n",
      "937/937 [==============================] - 20s - loss: 0.0491 - acc: 0.9860 - val_loss: 0.0187 - val_acc: 0.9950\n",
      "Epoch 25/25\n",
      "937/937 [==============================] - 20s - loss: 0.0463 - acc: 0.9867 - val_loss: 0.0182 - val_acc: 0.9950\n",
      "Test loss: 0.0125309597412\n",
      "Test accuracy: 0.9963\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras import backend as K\n",
    "from keras.utils.np_utils import to_categorical # convert to one-hot-encoding\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "\n",
    "batch_size = 64\n",
    "num_classes = 10\n",
    "epochs = 25\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters = 64, kernel_size = (3, 3), activation='relu',\n",
    "                 input_shape = (28, 28, 1)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(filters = 16, kernel_size = (3, 3), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(strides=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(filters = 32, kernel_size = (3, 3), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(filters = 32, kernel_size = (3, 3), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(strides=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#applying image augmentation\n",
    "gen = ImageDataGenerator(rotation_range=8, width_shift_range=0.08, shear_range=0.3,\n",
    "                         height_shift_range=0.08, zoom_range=0.08)\n",
    "\n",
    "test_gen = ImageDataGenerator()\n",
    "\n",
    "train_generator = gen.flow(x_train[0:45000], y_train[0:45000], batch_size=batch_size)\n",
    "test_generator = test_gen.flow(x_train[45000:60000], y_train[45000:60000], batch_size=batch_size)\n",
    "\n",
    "model.fit_generator(train_generator, steps_per_epoch=60000//batch_size, epochs=epochs, \n",
    "                    validation_data=test_generator, validation_steps=10000//batch_size,verbose=1)\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
